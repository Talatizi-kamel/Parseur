nom de fichier :marcu_statistics_sentence_pass_one.pdf convertis a txt
titre :Statistics-Based Summarization — Step One: Sentence Compression Kevin Knight and Daniel Marcu Information Sciences Institute and Department of Computer Science University of Southern California Abstract When humans produce summaries of documents, they Rather, they create new sentences that are grammati- cal, that cohere with one another, and that capture the most salient pieces of information in the original doc- pairs are available online, it is now possible to envision this paper, we focus on sentence compression, a sim- two goals simultaneously: our compressions should be grammatical, and they should retain the most impor- approaches to the problem, and we evaluate results Introduction Most of the research in automatic summarization has focused on extraction, portant textual segments is only half of what a summa- rization system needs to do because, in most cases, the simple catenation of textual segments does not yield have started to address the problem of generating co- the context of revising single document extracts; and The approach proposed by Witbrock and Mit- However, this model has yet to scale up to generat- ing multiple-sentence abstracts as well as well-formed, sets of manually written or semi-automatically derived rules for deleting information that is redundant, com- pressing long sentences into shorter ones, aggregating ever, in contrast with the above work, we intend to available, in order to automatically learn how to rewrite in the statistical MT community, which is focused on sentence-to-sentence translations, we also decided to fo- cus ﬁrst on a simpler problem, that of sentence compres- • First, the problem is complex enough to require the development of sophisticated compression models: Determining what is important in a sentence and determining how to convey the important informa- tion grammatically, using only a few words, is just a scaled down version of the text summarization prob- not have to worry yet about discourse related issues, • Second, an adequate solution to this problem has example, due to time and space constraints, the generation of TV captions often requires only the most important parts of sentences to be shown on a A good sentence compression module would there- fore have an impact on the task of automatic cap- can also be used to provide audio scanning ser- In gen- eral, since all systems aimed at producing coher- ent abstracts implement manually written sets of compression module would impact the overall quality In this paper, we present two approaches to the sen- velops a probabilistic noisy-channel model for sentence A noisy-channel model for sentence This section describes a probabilistic approach to the channel framework that has been relatively successful in a number of other NLP applications, including speech In this framework, we look at a long string and imag- Compression is a matter of identifying the original short machine translation, we look at a French string and say, “This was originally English, but someone added ‘noise’ from English originally, but by removing the noise, we can hypothesize an English source—and thereby trans- consists of optional text material that pads out the core be useful to imagine a scenario in which a news editor composes a short document, hands it to a reporter, and we may not have access to the editor’s original version As in any noisy channel application, we must solve three problems: erated as an “original short string” in the above hy- that when the short string s is expanded, the result as s except for the extra word “not,” then we may is equivalent to searching for the s that maximizes It is advantageous to break the problem down this way, as it decouples the somewhat independent goals build a channel model that focuses exclusively on the is, we can specify that a certain substring may represent unimportant information, but we do not need to worry that deleting it will result in an ungrammatical struc- use of extensive prior work in source language modeling for speech recognition, machine translation, and natu- re-use generic software packages to solve problems in all 
Les auteurs : 
Statistical Models
Training Corpus
Leurs adresses : 
Abstract : uman-written Abstract sentences are

cal; (ii) the Abstract sentences represent in a
compressed form the salient points of the original news
paper Sentences. We decided to keep in the corpus un
compressed sentences as well, since we want to learn 
not only how to compress a sentence, but also when to 
do it. 

(VP . . . ) 

Learning Model Parameters 
We collect expansion-template probabilities from our 
parallel corpus. We ﬁrst parse both sides of the parallel 
corpus, and then we identify corresponding syntactic 
nodes. For example, the parse tree for one sentence 
may begin (S (NP . . . ) 
(PP . . . )) while 
the parse tree for its compressed version may begin (S 
(NP . . . ) (VP . . . )). If these two S nodes are deemed 
to correspond, then we chalk up one joint event (S → 
NP VP, S → NP VP PP); afterwards we normalize. 
Not all nodes have corresponding partners; some non
correspondences are due to incorrect parses, while oth
ers are due to legitimate reformulations that are beyond 
the scope of our simple channel model. We use standard 
methods to estimate word-bigram probabilities. 

Decoding 
There is a vast number of potential compressions of a 
large tree t, but we can pack them all eﬃciently into a 
shared-forest structure. For each node of t that has n 
children, we 
• generate 2n − 1 new nodes, one for each non-empty 
• pack those nodes so that they are referred to as a 

subset of the children, and 

whole. 

For example, consider the large tree t above. All com
pressions can be represented with the following forest: 

A → B C H → a 
G → H A B → R 
A → C 
Q → Z 
C → b 
G → H 
Z → c 
A → C B D A → B 
G → A 
B → Q R A → C B 
A → D 
R → d 
D → e 
A → C D 
B → Q 
We can also assign an expansion-template probability 
to each node in the forest. For example, to the B → 
Q node, we can assign P(B → Q R | B → Q). If the 
observed probability from the parallel corpus is zero, 
then we assign a small ﬂoor value of 10−6. In reality, 
we produce forests that are much slimmer, as we only 
consider compressing a node in ways that are locally 
grammatical according to the Penn Treebank—if a rule 
of the type A → C B has never been observed, then it 
will not appear in the forest. 

At this point, we want to extract a set of high
scoring trees from the forest, taking into account 
both expansion-template probabilities and word-bigram 
probabilities. Fortunately, we have such a generic ex
tractor on hand (Langkilde 2000). This extractor was 
designed for a hybrid symbolic-statistical natural lan
guage generation system called Nitrogen. In that ap
plication, a rule-based component converts an abstract 

semantic representation into a vast number of potential 
English renderings. These renderings are packed into 
a forest, from which the most promising sentences are 
extracted using statistical scoring. 

For our purposes, the extractor selects the trees with 
the best combination of word-bigram and expansion
template scores. It returns a list of such trees, one for 
each possible compression length. For example, for 
the sentence Beyond that basic level, the operations of 
the three products vary, we obtain the following “best” 
compressions, with negative log-probabilities shown in 
parentheses (smaller = more likely): 

Beyond that basic level, the operations of the three products vary 

widely (1514588) 

Beyond that level, the operations of the three products vary widely 

(1430374) 

Beyond that basic level, the operations of the three products vary 

(1333437) 

Beyond that 

level, 

the operations of 

the three products vary 

(1249223) 

Beyond that basic level, 

the operations of 

the products vary 

(1181377) 

The operations of the three products vary widely (939912) 

The operations of the products vary widely (872066) 

The operations of the products vary (748761) 

The operations of products vary (690915) 

Operations of products vary (809158) 

The operations vary (522402) 

Operations vary (662642) 

Length Selection 
It is useful to have multiple answers to choose from, as 
one user may seek a 20% compression, while another 
seeks a 60% compression. However, for purposes of 
evaluation, we want our system to be able to select a 
single compression. If we rely on the log-probabilities 
as shown above, we will almost always choose the short
est compression. (Note above, however, how the three
word compression scores better than the two-word com
pression, as the models are not entirely happy removing 
the article “the”). To create a more fair competition, 
we divide the log-probability by the length of the com
pression, rewarding longer strings. This is commonly 
done in speech recognition. 

If we plot this normalized score against compression 
length, we usually observe a (bumpy) U-shaped curve, 
as illustrated in Figure 3. 
In a typical more diﬃcult 
case, a 25-word sentence may be optimally compressed 
by a 17-word version. Of course, if a user requires a 
shorter compression than that, she may select another 
region of the curve and look for a local minimum. 

A decision-based model for sentence 

compression 

In this section, we describe a decision-based, history 
model of sentence compression. As in the noisy-channel 
approach, we again assume that we are given as input 

  

t 
s 
e 
b 
  
f 
o 
y 
t 
i 
l 
i 
b 
a 
b 
o 
r 
p 

g 
o 
l 
  
e 
v 
i 
t 
a 
g 
e 
n 
d 
e 
t 
s 
u 
j 
d 
A 

  

  

n 
h 
t 
g 
n 
e 
l 
  
r 
a 
l 
u 
c 
i 
t 
r 
a 
p 

  
a 
  
t 
a 
  
s 
  
n 
o 
i 
s 
s 
e 
r 
p 
m 
o 
c 

n 

  
/ 
  
) 
s 
  
| 
  
t 
  
( 
P 
  
) 
s 
( 
P 
g 
o 
l 

  
  
  
  
  
  
  
  
  
  
  
  
  
  

  

. 
  
e 
c 
n 
a 
t 
s 
i 
d 
  
s 
i 
  
d 
n 
a 
b 
d 
a 
o 
r 
b 
  
f 
o 

  
e 
g 
a 
t 
n 
a 
v 
d 
a 
  
r 
e 
h 
t 
o 
n 
a 
  
y 
l 
l 
a 
n 
i 

F 

. 
  
e 
c 
n 
a 
t 
s 
i 
d 

  
s 
i 
  

d 
n 
a 
b 
d 
a 
o 
r 
b 

  
f 
o 
  
e 
g 
a 
t 
n 
a 
v 
d 
a 
  
r 
e 
h 
t 
o 
n 
a 
  
, 

y 
l 
l 
a 
n 
i 

F 

. 
  
e 
c 
n 
a 
t 
s 
i 
d 
  
s 
i 
  
e 
g 
a 
t 
n 
a 
v 
d 
A 

. 
  
e 
c 
n 
a 
t 
s 
i 
d 
  
s 
i 
  
e 
g 
a 
t 
n 
a 
v 
d 
a 
  
r 
e 
h 
t 
o 
n 
A 

. 
  
e 
c 
n 
a 
t 
s 
i 
d 

  
s 
i 
  

d 
n 
a 
b 
d 
a 
o 
r 
b 

  
f 
o 

  
e 
g 
a 
t 
n 
a 
v 
d 
A 

. 
  
e 
c 
n 
a 
t 
s 
i 
d 

  
s 
i 
  
d 
n 
a 
b 
d 
a 
o 
r 
b 

  
f 
o 

  
e 
g 
a 
t 
n 
a 
v 
d 
a 
  
r 
e 
h 
t 
o 
n 
A 

4 

5 

6 

7 

8 

9 

Compression length n 

0.20 

0.15 

0.10 

Figure 3: Adjusted log-probabilities for top-scoring 
compressions at various lengths (lower is better). 

a parse tree t. Our goal is to “rewrite” t into a smaller 
tree s, which corresponds to a compressed version of the 
original sentence subsumed by t. Suppose we observe in 
our corpus the trees t and s2 in Figure 1. In this model, 
we ask ourselves how we may go about rewriting t into 
s2. One possible solution is to decompose the rewriting 
operation into a sequence of shift-reduce-drop actions 
that are speciﬁc to an extended shift-reduce parsing 
paradigm. 

In the model we propose, the rewriting process starts 
with an empty Stack and an Input List that contains the 
sequence of words subsumed by the large tree t. Each 
word in the input list is labeled with the name of all syn
tactic constituents in t that start with it (see Figure 4). 
At each step, the rewriting module applies an opera
tion that is aimed at reconstructing the smaller tree s2. 
In the context of our sentence-compression module, we 
need four types of operations: 
• shift operations transfer the ﬁrst word from the in
• reduce operations pop the k syntactic trees located 
at the top of the stack; combine them into a new 
tree; and push the new tree on the top of the stack. 
Reduce operations are used to derive the structure of 
the syntactic tree of the short sentence. 
• drop operations are used to delete from the input list 
subsequences of words that correspond to syntactic 
constituents. A drop x operations deletes from the 

put list into the stack; 

Stack 

Input List 

Stack 

Input List 

G 
H 
a 

A 
C 
b 

B 
Q 
Z 
c 

R 
d 

D 
e 

A 
C 
b 

B 
Q 
Z 
c 

R 
d 

B 
Q 
Z 
c 

R 
d 

D 
e 

H 

a 

K 

b 

H 

a 

D 
e 

SHIFT; 
ASSIGNTYPE H 

STEPS 1-2 

SHIFT; 
ASSIGNTYPE K 

STEPS 3-4 

REDUCE 2 F 

STEP 5 

D 
e 

R 
d 

B 
Q 
Z 
c 

D 
e 

F 

K 

b 

F 

K 

b 

D 

e 

D 

e 

G 

H 

a 

F 

F 

H 

a 

K 

b 

K 

b 

H 

a 

H 

a 

DROP B 

STEP 6 

SHIFT; 
ASSIGNTYPE D 

STEPS 7-8 

REDUCE 2 G 

STEP 9 

Figure 4: Example of incremental tree compression. 

input list all words that are spanned by constituent 
x in t. 
• assignType operations are used to change the label 
of trees at the top of the stack. These actions assign 
POS tags to the words in the compressed sentence, 
which may be diﬀerent from the POS tags in the 
original sentence. 

The decision-based model 
is more ﬂexible than the 
channel model because it enables the derivation of trees 
whose skeleton can diﬀer quite drastically from that of 
the tree given as input. For example, using the channel 
model, we are unable to obtain tree s2 from t. However, 
the four operations listed above enable us to rewrite a 
tree t into any tree s, as long as an in-order traversal of 
the leaves of s produces a sequence of words that occur 
in the same order as the words in the tree t. For exam
ple, the tree s2 can be obtained from tree t by following 
this sequence of actions, whose eﬀects are shown in Fig
ure 4: shift; assignType H; shift; assignType K; 
reduce 2 F; drop B; shift; assignType D; reduce 
2 G. 

To save space, we show shift and assignType op
erations on the same line; however, the reader should 
understand that they correspond to two distinct ac
tions. As one can see, the assignType K operation 
rewrites the POS tag of the word b; the reduce op
erations modify the skeleton of the tree given as input. 
To increase readability, the input list is shown in a for
mat that resembles as closely as possible the graphical 
representation of the trees in ﬁgure 1. 

Learning the parameters of the 
decision-based model 
We associate with each conﬁguration of our shift
reduce-drop, rewriting model a learning case. The cases 
are generated automatically by a program that derives 
sequences of actions that map each of the large trees in 
our corpus into smaller trees. The rewriting procedure 
simulates a bottom-up reconstruction of the smaller 
trees. 

Overall, the 1067 pairs of long and short sentences 
yielded 46383 learning cases. Each case was labeled 

with one action name from a set of 210 possible ac
tions: There are 37 distinct assignType actions, one 
for each POS tag. There are 63 distinct drop actions, 
one for each type of syntactic constituent that can be 
deleted during compression. There are 109 distinct re
duce actions, one for each type of reduce operation that 
is applied during the reconstruction of the compressed 
sentence. And there is one shift operation. Given a 
tree t and an arbitrary conﬁguration of the stack and 
input list, the purpose of the decision-based classiﬁer 
is to learn what action to choose from the set of 210 
possible actions. 

To each learning example, we associated a set of 99 

features from the following two classes: 
Operational features reﬂect the number of trees 
in the stack, 
the input list, and the types of 
the last ﬁve operations. They also encode infor
mation that denote the syntactic category of the 
root nodes of the partial trees built up to a cer
tain time. Examples of such features are: num
berTreesInStack, wasPreviousOperationShift, 
syn
tacticLabelOfTreeAtTheTopOfStack, etc. 

Original-tree-speciﬁc features denote the 

syntac
tic constituents that start with the ﬁrst unit in the 
input list. Examples of such features are: inputList
StartsWithA CC, inputListStartsWithA PP, etc. 
The decision-based compression module uses the 
C4.5 program (Quinlan 1993) in order to learn deci
sion trees that specify how large syntactic trees can 
be compressed into shorter trees. A ten-fold cross
validation evaluation of the classiﬁer yielded an accu
racy of 87.16% (± 0.14). A majority baseline classi
ﬁer that chooses the action shift has an accuracy of 
28.72%. 

Employing the decision-based model 
To compress sentences, we apply the shift-reduce-drop 
model in a deterministic fashion. We parse the sentence 
to be compressed (Collins 1997) and we initialize the 
input list with the words in the sentence and the syn
tactic constituents that “begin” at each word, as shown 
in Figure 4. We then incrementally inquire the learned 
classiﬁer what action to perform, and we simulate the 
execution of that action. The procedure ends when the 
input list is empty and when the stack contains only 
one tree. An inorder traversal of the leaves of this tree 
produces the compressed version of the sentence given 
as input. 

Since the model is deterministic, it produces only one 
output. The advantage is that the compression is very 
fast: it takes only a few milliseconds per sentence. The 
disadvantage is that it does not produce a range of 
compressions, from which another system may subse
quently choose. It is straightforward though to extend 
the model within a probabilistic framework by applying, 
for example, the techniques used by Magerman (1995). 

Evaluation 

To evaluate our compression algorithms, we randomly 
selected 32 sentence pairs from our parallel corpus, 
which we will refer to as the Test Corpus. We used the 
other 1035 sentence pairs for training. Figure 5 shows 
three sentences from the Test Corpus, together with the 
compressions produced by humans, our compression al
gorithms, and a baseline algorithm that produces com
pressions with highest word-bigram scores. The exam
ples are chosen so as to reﬂect good, average, and bad 
performance cases. The ﬁrst sentence is compressed in 
the same manner by humans and our algorithms (the 
baseline algorithm chooses though not to compress this 
sentence). For the second example, the output of the 
Decision-based algorithm is grammatical, but the se
mantics is negatively aﬀected. The noisy-channel al
gorithm deletes only the word “break”, which aﬀects 
the correctness of the output less. In the last example, 
the noisy-channel model is again more conservative and 
decides not to drop any constituents. In constrast, the 
decision-based algorithm compresses the input substan
tially, but it fails to produce a grammatical output. 

We presented each original sentence in the Test Cor
pus to four judges, together with four compressions of it: 
the human generated compression, the outputs of the 
noisy-channel and decision-based algorithms, and the 
output of the baseline algorithm. The judges were told 
that all outputs were generated automatically. The or
der of the outputs was scrambled randomly across test 
cases. 

To avoid confounding, the judges participated in two 
experiments. In the ﬁrst experiment, they were asked 
to determine on a scale from 1 to 5 how well the systems 
did with respect to selecting the most important words 
in the original sentence. In the second experiment, they 
were asked to determine on a scale from 1 to 5 how 
grammatical the outputs were. 

We also investigated how sensitive our algorithms are 
with respect to the training data by carrying out the 
same experiments on sentences of a diﬀerent genre, the 
scientiﬁc one. To this end, we took the ﬁrst sentence of 
the ﬁrst 26 articles made available in 1999 on the cmplg 
archive. We created a second parallel corpus, which 
we will refer to as the Cmplg Corpus, by generating 
by ourselves compressed grammatical versions of these 
sentences. Since some of the sentences in this corpus 
were extremely long, the baseline algorithm could not 
produce compressed versions in reasonable time. 

The results in Table 1 show compression rates, and 
mean and standard deviation results across all judges, 
for each algorithm and corpus. The results show that 
the decision-based algorithm is the most aggressive: 
on average, it compresses sentences to about half of 
their original size. The compressed sentences produced 
by both algorithms are more “grammatical” and con
tain more important words than the sentences pro
duced by the baseline. T -test experiments showed these 
diﬀerences to be statistically signiﬁcant at p < 0.01 
both for individual judges and for average scores across 

Beyond the basic level, the operations of the three products vary widely. 
Beyond the basic level, the operations of the three products vary widely. 

Original: 
Baseline: 
Noisy-channel: The operations of the three products vary widely. 
Decision-based: The operations of the three products vary widely. 
Humans: 
The operations of the three products vary widely. 

Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles. 
Arborscan and worked in, but it very large dxf. 

Original: 
Baseline: 
Noisy-channel: Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles. 
Decision-based: Arborscan is reliable and worked accurately in testing very large dxf ﬁles. 
Humans: 

Arborscan produces very large dxf ﬁles. 

Original: 

Many debugging features, including user-deﬁned break points and variable-watching and 
message-watching windows, have been added. 
Baseline: 
Debugging, user-deﬁned and variable-watching and message-watching, have been. 
Noisy-channel: Many debugging features, including user-deﬁned points and variable-watching and 

message-watching windows, have been added. 

Decision-based: Many debugging features. 
Humans: 

Many debugging features have been added . 

Figure 5: Compression examples 

Corpus Avg. orig. sent. length 
Test 

21 words 

Cmplg 

26 words 

Compression 
Grammaticality 
Importance 
Compression 
Grammaticality 
Importance 

Baseline 
63.70% 
1.78±1.19 
2.17±0.89 

– 
– 
– 

Noisy-channel Decision-based 
70.37% 
4.34±1.02 
3.38±0.67 
65.68% 
4.22±0.99 
3.42±0.97 

57.19% 
4.30±1.33 
3.54±1.00 
54.25% 
3.72±1.53 
3.24±0.68 

Humans 
53.33% 
4.92±0.18 
4.24 ±0.52 
65.68% 
4.97±0.08 
4.32±0.54 

Table 1: Experimental results 

all judges. T -tests showed no signiﬁcant statistical 
diﬀerences between the two algorithms. As Table 1 
shows, the performance of the compression algorithms 
is much closer to human performance than baseline per
formance; yet, humans perform statistically better than 
our algorithms at p < 0.01. 

When applied to sentences of a diﬀerent genre, the 
performance of the noisy-channel compression algo
rithm degrades smoothly, while the performance of the 
decision-based algorithm drops sharply. This is due to 
a few sentences in the Cmplg Corpus that the decision
based algorithm over-compressed to only two or three 
words. We suspect that this problem can be ﬁxed if 
the decision-based compression module is extended in 
the style of Magerman (1995), by computing probabil
ities across the sequences of decisions that correspond 
to a compressed sentence. Likewise, there are substan
tial gains to be had in noisy-channel modeling—we see 
clearly in the data many statistical dependencies and 
processes that are not captured in our simple initial 
models. More grammatical output will come from tak
ing account of subcategory and head-modiﬁer statistics 
(in addition to simple word-bigrams), and an expanded 
channel model will allow for more tree manipulation 
possibilities. Work on extending the algorithms pre
sented in this paper to compressing multiple sentences 
is currently underway. 


Introduction : 
Corps : 
Conclusion : 
Discussion : 
Biblio : 
Ref :Barzilay, R.; McKeown, K.; and Elhadad, M. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL–99), 550–557. Berger, A., and Laﬀerty, J. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99), 222–229. Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics 19(2):263–311. Church, K. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, 136–143. Collins, M. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL–97), 16–23. 1998. Producing intelligent teleGrefenstette, G. graphic text reduction to provide an audio scanning service for the blind. In Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, 111–118. Jelinek, F. 1997. Statistical Methods for Speech Recognition. The MIT Press. Jing, H., and McKeown, K. 1999. The decomposition of human-written summary sentences. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99). Knight, K., and Graehl, J. 1998. Machine transliteration. Computational Linguistics 24(4):599–612. Langkilde, I. 2000. Forest-based statistical sentence generation. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics. 1999. Closed captioning in AmerLinke-Ellis, N. ica: Looking beyond compliance. In Proceedings of the TAO Workshop on TV Closed Captions for the hearing impaired people, 43–59. Magerman, D. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, 276– 283. Mani, I., and Maybury, M., eds. 1999. Advances in Automatic Text Summarization. The MIT Press. Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improving summaries by revising them. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, 558–565. McKeown, K.; Klavans, J.; Hatzivassiloglou, V.; Barzilay, R.; and Eskin, E. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of the Sixteenth National Conference on Artiﬁcial Intelligence (AAAI–99). Quinlan, J. 1993. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann Publishers. Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burnham, D. 1999. Semi-automatic captioning of TV programs, an Australian perspective. In Proceedings of the TAO Workshop on TV Closed Captions for the hearing impaired people, 87–100. Witbrock, M., and Mittal, V. Ultrasummarization: A statistical approach to generating highly condensed non-extractive summaries. In Proceedings of the 22nd International Conference on Research and Development in Information Retrieval (SIGIR’99), Poster Session, 315–316. 1999.  
